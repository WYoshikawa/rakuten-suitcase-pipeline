#!/usr/bin/env python3
"""
ğŸ¦ RASCAL 3.0å¯¾å¿œ rakuten_rank_step1.pyã€å®Œå…¨æ›¸ãæ›ãˆç‰ˆã€‘
-----------------------------------
â–¸ æ¥½å¤© Item Ranking API (ã‚¹ãƒ¼ãƒ„ã‚±ãƒ¼ã‚¹: genreId=301577) ã‹ã‚‰ä¸Šä½ 1000 ä»¶å–å¾—
â–¸ åŸºæœ¬CSVå‡ºåŠ›ã¯å¿…é ˆã€--with-imagesã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ç”»åƒURLè¿½åŠ 
â–¸ å•†å“åã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è©³ç´°æ©Ÿèƒ½ãƒ•ãƒ©ã‚°ä»˜ä¸ï¼ˆ30ç¨®é¡ã®æ©Ÿèƒ½æ¤œå‡ºï¼‰
â–¸ GitHub Actionså¯¾å¿œãƒ»ã‚¨ãƒ©ãƒ¼è€æ€§å¼·åŒ–

ä½¿ã„æ–¹:
    1. .env ã« APP_ID=your_application_id ã‚’è¨­å®š
    2. pip install requests pandas tqdm python-dotenv
    3. python rakuten_rank_step1.py --pages 10                    # åŸºæœ¬ç‰ˆ
    4. python rakuten_rank_step1.py --pages 10 --with-images     # ç”»åƒå¯¾å¿œç‰ˆ
"""
import os, time, argparse, re
from datetime import date
import requests, pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv

load_dotenv()
APP_ID = os.getenv("APP_ID", "").strip()
if not APP_ID:
    raise RuntimeError("âŒ APP_ID missing (.env or export APP_ID=xxxx)")

GENRE_ID  = 301577
HEADERS   = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# ========== æ©Ÿèƒ½æ¤œå‡ºç”¨æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ ==========

# æ—¢å­˜æ©Ÿèƒ½
USB_RE    = re.compile(r"USB|ãƒãƒ¼ãƒˆ", re.I)
EXPAND_RE = re.compile(r"æ‹¡å¼µ|ã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ‰", re.I)
FRONT_RE  = re.compile(r"ãƒ•ãƒ­ãƒ³ãƒˆ|å‰é–‹ã", re.I)

# ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç³»
TSA_RE = re.compile(r"TSA|TSAãƒ­ãƒƒã‚¯", re.I)
LOCK_RE = re.compile(r"ãƒ­ãƒƒã‚¯(?!TSA)|éµ", re.I)  # TSAãƒ­ãƒƒã‚¯ä»¥å¤–ã®ãƒ­ãƒƒã‚¯

# âš–ï¸ é‡é‡ãƒ»ç´ æç³»
LIGHTWEIGHT_RE = re.compile(r"è»½é‡|è¶…è»½|è»½ã„", re.I)
HARDCASE_RE = re.compile(r"ãƒãƒ¼ãƒ‰|ãƒãƒ¼ãƒ‰ã‚±ãƒ¼ã‚¹", re.I)
SOFTCASE_RE = re.compile(r"ã‚½ãƒ•ãƒˆ|ã‚½ãƒ•ãƒˆã‚±ãƒ¼ã‚¹", re.I)

# ğŸ› ã‚­ãƒ£ã‚¹ã‚¿ãƒ¼ãƒ»ç§»å‹•ç³»
SILENT_RE = re.compile(r"é™éŸ³|ã‚µã‚¤ãƒ¬ãƒ³ãƒˆ", re.I)
WHEEL360_RE = re.compile(r"360åº¦|360Â°", re.I)
FOUR_WHEEL_RE = re.compile(r"4è¼ª|å››è¼ª", re.I)
STOPPER_RE = re.compile(r"ã‚¹ãƒˆãƒƒãƒ‘ãƒ¼", re.I)

# ğŸ“ ã‚µã‚¤ã‚ºãƒ»å®¹é‡ç³»
CARRY_ON_RE = re.compile(r"æ©Ÿå†…æŒã¡è¾¼ã¿|æ©Ÿå†…æŒè¾¼", re.I)
LARGE_CAPACITY_RE = re.compile(r"å¤§å®¹é‡", re.I)

# ğŸ”§ ãã®ä»–æ©Ÿèƒ½
SHOCK_RESISTANT_RE = re.compile(r"è€è¡æ’ƒ|è¡æ’ƒã«å¼·ã„", re.I)
CUP_HOLDER_RE = re.compile(r"ã‚«ãƒƒãƒ—ãƒ›ãƒ«ãƒ€ãƒ¼|ãƒ‰ãƒªãƒ³ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼", re.I)

# ğŸ·ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰ç³»
SAMSONITE_RE = re.compile(r"ã‚µãƒ ã‚½ãƒŠã‚¤ãƒˆ|SAMSONITE", re.I)
INNOVATOR_RE = re.compile(r"ã‚¤ãƒãƒ™ãƒ¼ã‚¿ãƒ¼|innovator", re.I)

# ğŸ›¡ï¸ è£œå„Ÿãƒ»ä¿è¨¼ç³»
WARRANTY_RE = re.compile(r"ä¿è¨¼|è£œå„Ÿ|warranty|guarantee", re.I)
WARRANTY_1YEAR_RE = re.compile(r"1å¹´ä¿è¨¼", re.I)
WARRANTY_2YEAR_RE = re.compile(r"2å¹´ä¿è¨¼", re.I)
WARRANTY_3YEAR_RE = re.compile(r"3å¹´ä¿è¨¼|å“è³ªä¿è¨¼ç²å¾—", re.I)

# ğŸ¨ å®¿æ³Šæ—¥æ•°ç³»
STAY_1NIGHT_RE = re.compile(r"1æ³Š", re.I)
STAY_2NIGHT_RE = re.compile(r"2æ³Š", re.I)
STAY_3NIGHT_RE = re.compile(r"3æ³Š", re.I)
STAY_4NIGHT_RE = re.compile(r"4æ³Š", re.I)
STAY_5NIGHT_RE = re.compile(r"5æ³Š", re.I)
STAY_SHORT_RE = re.compile(r"1æ³Š2æ—¥|2æ³Š3æ—¥|çŸ­æœŸ", re.I)
STAY_LONG_RE = re.compile(r"4æ³Š5æ—¥|5æ³Š|é•·æœŸ", re.I)

# ğŸ“¢ ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°è¨´æ±‚ãƒ‘ã‚¿ãƒ¼ãƒ³
# ğŸ’° ä¾¡æ ¼è¨´æ±‚
PRICE_APPEAL_RE = re.compile(r"OFF|ã‚¯ãƒ¼ãƒãƒ³|å‰²|æœ€å®‰|æ¿€å®‰|æ ¼å®‰|è¡æ’ƒä¾¡æ ¼|ã‚»ãƒ¼ãƒ«", re.I)
# ğŸ“º æ¨©å¨ãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢è¨´æ±‚
AUTHORITY_APPEAL_RE = re.compile(r"TV|ãƒ†ãƒ¬ãƒ“|ãƒ¡ãƒ‡ã‚£ã‚¢|ç´¹ä»‹|æ­£è¦å“|ãƒ©ãƒ³ã‚­ãƒ³ã‚°|1ä½|å—è³|CAç›£ä¿®|æ¥½å¤©1ä½", re.I)
# âš¡ æ©Ÿèƒ½ãƒ»æ€§èƒ½è¨´æ±‚
FUNCTION_APPEAL_RE = re.compile(r"è¶…è»½é‡|å¤šæ©Ÿèƒ½|é«˜æ©Ÿèƒ½|æœ€æ–°|ç‰¹è¨±|ç‹¬è‡ª|é©æ–°|é€²åŒ–", re.I)
# ğŸƒ ç·Šæ€¥ãƒ»é™å®šè¨´æ±‚
URGENCY_APPEAL_RE = re.compile(r"é™å®š|å…ˆç€|24Hé™å®š|ä»Šã ã‘|å†å…¥è·|åœ¨åº«é™ã‚Š|æœŸé–“é™å®š", re.I)

def rank_url(page):
    """æ¥½å¤©ãƒ©ãƒ³ã‚­ãƒ³ã‚°API URLç”Ÿæˆ"""
    return (f"https://app.rakuten.co.jp/services/api/IchibaItem/Ranking/20220601"
            f"?applicationId={APP_ID}&format=json&genreId={GENRE_ID}&page={page}")

def analyze_features(name):
    """å•†å“åã‹ã‚‰å„ç¨®æ©Ÿèƒ½ãƒ•ãƒ©ã‚°ã‚’æ¤œå‡º"""
    return {
        # æ—¢å­˜æ©Ÿèƒ½
        "has_USB": bool(USB_RE.search(name)),
        "has_expand": bool(EXPAND_RE.search(name)),
        "has_frontOP": bool(FRONT_RE.search(name)),
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
        "has_TSA": bool(TSA_RE.search(name)),
        "has_lock": bool(LOCK_RE.search(name)),
        
        # é‡é‡ãƒ»ç´ æ
        "has_lightweight": bool(LIGHTWEIGHT_RE.search(name)),
        "has_hardcase": bool(HARDCASE_RE.search(name)),
        "has_softcase": bool(SOFTCASE_RE.search(name)),
        
        # ã‚­ãƒ£ã‚¹ã‚¿ãƒ¼ãƒ»ç§»å‹•
        "has_silent": bool(SILENT_RE.search(name)),
        "has_360wheel": bool(WHEEL360_RE.search(name)),
        "has_4wheel": bool(FOUR_WHEEL_RE.search(name)),
        "has_stopper": bool(STOPPER_RE.search(name)),
        
        # ã‚µã‚¤ã‚ºãƒ»å®¹é‡
        "has_carry_on": bool(CARRY_ON_RE.search(name)),
        "has_large_capacity": bool(LARGE_CAPACITY_RE.search(name)),
        
        # ãã®ä»–æ©Ÿèƒ½
        "has_shock_resistant": bool(SHOCK_RESISTANT_RE.search(name)),
        "has_cup_holder": bool(CUP_HOLDER_RE.search(name)),
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰
        "is_samsonite": bool(SAMSONITE_RE.search(name)),
        "is_innovator": bool(INNOVATOR_RE.search(name)),
        
        # è£œå„Ÿãƒ»ä¿è¨¼
        "has_warranty": bool(WARRANTY_RE.search(name)),
        "has_1year_warranty": bool(WARRANTY_1YEAR_RE.search(name)),
        "has_2year_warranty": bool(WARRANTY_2YEAR_RE.search(name)),
        "has_3year_warranty": bool(WARRANTY_3YEAR_RE.search(name)),
        
        # å®¿æ³Šæ—¥æ•°
        "for_1night": bool(STAY_1NIGHT_RE.search(name)),
        "for_2night": bool(STAY_2NIGHT_RE.search(name)),
        "for_3night": bool(STAY_3NIGHT_RE.search(name)),
        "for_4night": bool(STAY_4NIGHT_RE.search(name)),
        "for_5night": bool(STAY_5NIGHT_RE.search(name)),
        "for_short_stay": bool(STAY_SHORT_RE.search(name)),
        "for_long_stay": bool(STAY_LONG_RE.search(name)),
        
        # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°è¨´æ±‚ãƒ‘ã‚¿ãƒ¼ãƒ³
        "appeal_price": bool(PRICE_APPEAL_RE.search(name)),
        "appeal_authority": bool(AUTHORITY_APPEAL_RE.search(name)),
        "appeal_function": bool(FUNCTION_APPEAL_RE.search(name)),
        "appeal_urgency": bool(URGENCY_APPEAL_RE.search(name)),
    }

def extract_image_url(item, image_size="medium"):
    """ğŸ¨ å•†å“ç”»åƒURLæŠ½å‡ºï¼ˆRASCAL 3.0å¯¾å¿œãƒ»ã‚¨ãƒ©ãƒ¼è€æ€§å¼·åŒ–ï¼‰"""
    try:
        # ç”»åƒã‚µã‚¤ã‚ºåˆ¥URLå–å¾—ï¼ˆå„ªå…ˆé †ä½ã‚ã‚Šï¼‰
        url_candidates = []
        
        # 1. mediumImageUrlsï¼ˆé…åˆ—ï¼‰
        if image_size == "medium" and "mediumImageUrls" in item:
            image_urls = item["mediumImageUrls"]
            if image_urls and len(image_urls) > 0:
                url_candidates.append(image_urls[0].get("imageUrl", ""))
        
        # 2. smallImageUrlsï¼ˆé…åˆ—ï¼‰
        elif image_size == "small" and "smallImageUrls" in item:
            image_urls = item["smallImageUrls"]
            if image_urls and len(image_urls) > 0:
                url_candidates.append(image_urls[0].get("imageUrl", ""))
        
        # 3. mediumImageUrlï¼ˆå˜ä¸€ï¼‰
        if "mediumImageUrl" in item and item["mediumImageUrl"]:
            url_candidates.append(item["mediumImageUrl"])
        
        # 4. smallImageUrlï¼ˆå˜ä¸€ï¼‰  
        if "smallImageUrl" in item and item["smallImageUrl"]:
            url_candidates.append(item["smallImageUrl"])
        
        # æœ‰åŠ¹ãªURLã‚’è¿”ã™
        for url in url_candidates:
            if url and url.startswith("http"):
                return url
        
        # ç”»åƒURLãŒãªã„å ´åˆã¯ç©ºæ–‡å­—
        return ""
        
    except Exception as e:
        print(f"âš ï¸ ç”»åƒURLæŠ½å‡ºã‚¨ãƒ©ãƒ¼ (rank {item.get('rank', '?')}): {e}")
        return ""

def fetch_ranking_data(pages, with_images=False):
    """æ¥½å¤©ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼è€æ€§å¼·åŒ–ï¼‰"""
    rows = []
    print(f"ğŸš€ æ¥½å¤©ã‚¹ãƒ¼ãƒ„ã‚±ãƒ¼ã‚¹ãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—é–‹å§‹ (ä¸Šä½{pages * 100}ä»¶)")
    if with_images:
        print("ğŸ¨ ç”»åƒURLå–å¾—æ©Ÿèƒ½: æœ‰åŠ¹")
    
    successful_pages = 0
    failed_pages = 0
    
    for p in tqdm(range(1, pages+1), desc="ğŸ“¡ ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­"):
        try:
            # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = requests.get(rank_url(p), headers=HEADERS, timeout=15)
            response.raise_for_status()
            
            js = response.json()
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç¢ºèª
            if "error" in js:
                print(f"âŒ API ã‚¨ãƒ©ãƒ¼ (page {p}): {js['error']}")
                failed_pages += 1
                continue
            
            items_in_page = js.get("Items", [])
            if not items_in_page:
                print(f"âš ï¸ ãƒšãƒ¼ã‚¸ {p}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                failed_pages += 1
                continue
            
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            for item_wrapper in items_in_page:
                try:
                    item = item_wrapper["Item"]
                    name = item["itemName"]
                    
                    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…é ˆï¼‰
                    row_data = {
                        "rank": item["rank"],
                        "itemCode": item["itemCode"],
                        "itemName": name,
                        "itemPrice": item["itemPrice"],
                        "reviewAverage": item.get("reviewAverage", 0),
                        "reviewCount": item.get("reviewCount", 0),
                        "itemUrl": item.get("itemUrl", ""),
                    }
                    
                    # ğŸ¨ ç”»åƒURLè¿½åŠ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    if with_images:
                        row_data["imageUrl"] = extract_image_url(item, "medium")
                        # å°ã•ã„ã‚µã‚¤ã‚ºã‚‚å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
                        small_url = extract_image_url(item, "small")
                        if small_url and not row_data["imageUrl"]:
                            row_data["imageUrl"] = small_url
                    
                    # æ©Ÿèƒ½ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ 
                    row_data.update(analyze_features(name))
                    rows.append(row_data)
                    
                except KeyError as e:
                    print(f"âš ï¸ å•†å“ãƒ‡ãƒ¼ã‚¿ä¸å‚™ (page {p}): {e}")
                    continue
                except Exception as e:
                    print(f"âš ï¸ å•†å“å‡¦ç†ã‚¨ãƒ©ãƒ¼ (page {p}): {e}")
                    continue
            
            successful_pages += 1
            
        except requests.exceptions.Timeout:
            print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (page {p})")
            failed_pages += 1
            continue
        except requests.exceptions.RequestException as e:
            print(f"âŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ (page {p}): {e}")
            failed_pages += 1
            continue
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ (page {p}): {e}")
            failed_pages += 1
            continue
            
        # APIè² è·è»½æ¸›
        time.sleep(1)
    
    print(f"ğŸ“Š å–å¾—çµæœ: æˆåŠŸ {successful_pages}ãƒšãƒ¼ã‚¸, å¤±æ•— {failed_pages}ãƒšãƒ¼ã‚¸")
    
    if not rows:
        raise RuntimeError("âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å®Œå…¨å¤±æ•—ã—ã¾ã—ãŸ")
    
    return pd.DataFrame(rows)

def display_statistics(df, with_images=False):
    """çµ±è¨ˆæƒ…å ±è¡¨ç¤º"""
    print(f"\nğŸ“Š æ©Ÿèƒ½åˆ¥çµ±è¨ˆ (å…¨{len(df)}ä»¶)")
    print("=" * 50)
    
    # æ©Ÿèƒ½çµ±è¨ˆ
    feature_cols = [col for col in df.columns if col.startswith(('has_', 'is_', 'for_', 'appeal_'))]
    feature_stats = []
    
    for col in feature_cols:
        try:
            count = df[col].sum() if df[col].dtype == bool else (df[col] == True).sum()
            percentage = (count / len(df)) * 100
            feature_name = col.replace('has_', '').replace('is_', '').replace('for_', '').replace('appeal_', '')
            feature_stats.append((feature_name, count, percentage))
        except Exception:
            continue
    
    # å‡ºç¾ç‡é †ã«ã‚½ãƒ¼ãƒˆ
    feature_stats.sort(key=lambda x: x[2], reverse=True)
    
    # ä¸Šä½15æ©Ÿèƒ½ã®ã¿è¡¨ç¤º
    for feature_name, count, percentage in feature_stats[:15]:
        print(f"{feature_name:15}: {count:3d}ä»¶ ({percentage:5.1f}%)")
    
    # ğŸ¨ ç”»åƒURLçµ±è¨ˆï¼ˆRASCAL 3.0å¯¾å¿œï¼‰
    if with_images:
        print(f"\nğŸ¨ ç”»åƒURLå–å¾—çµ±è¨ˆ")
        print("=" * 30)
        if 'imageUrl' in df.columns:
            valid_images = df['imageUrl'].notna().sum()
            non_empty_images = (df['imageUrl'] != '').sum()
            valid_percentage = (non_empty_images / len(df)) * 100
            print(f"ç”»åƒURLå–å¾—æˆåŠŸ: {non_empty_images}ä»¶ ({valid_percentage:.1f}%)")
            
            # ç”»åƒURLä¾‹ï¼ˆéç©ºã®ã‚‚ã®ï¼‰
            sample_images = df[df['imageUrl'].notna() & (df['imageUrl'] != '')]['imageUrl'].head(3)
            if len(sample_images) > 0:
                print(f"ç”»åƒURLä¾‹:")
                for i, url in enumerate(sample_images, 1):
                    print(f"  {i}. {url[:80]}...")
            else:
                print("  âš ï¸ æœ‰åŠ¹ãªç”»åƒURLãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            print("ç”»åƒURLå–å¾—: ç„¡åŠ¹")

def save_data(df, with_images=False):
    """ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆä¸¡æ–¹å¼å¯¾å¿œï¼‰"""
    today = date.today()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    if with_images:
        filename = f"rank_base_{today}_with_images.csv"
    else:
        filename = f"rank_base_{today}.csv"
    
    try:
        # CSVä¿å­˜
        df.to_csv(filename, index=False, encoding='utf-8')
        
        print(f"\nâœ… ä¿å­˜å®Œäº†: {filename}")
        print(f"ğŸ“ˆ å–å¾—ä»¶æ•°: {len(df)}ä»¶")
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(filename):,} bytes")
        
        # åˆ—æ•°è¡¨ç¤º
        feature_cols = [col for col in df.columns if col.startswith(('has_', 'is_', 'for_', 'appeal_'))]
        print(f"ğŸ·ï¸  æ©Ÿèƒ½ãƒ•ãƒ©ã‚°: {len(feature_cols)}ç¨®é¡")
        
        if with_images and 'imageUrl' in df.columns:
            valid_images = (df['imageUrl'] != '').sum()
            print(f"ğŸ¨ ç”»åƒURL: {valid_images}ä»¶å–å¾—")
            
            # RASCAL 3.0 åˆ†ææº–å‚™å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            if valid_images > 0:
                print(f"\nğŸ¦ RASCAL 3.0 ç”»åƒåˆ†ææº–å‚™å®Œäº†ï¼")
                print(f"æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§ç”»åƒåˆ†æã‚’å®Ÿè¡Œ:")
                print(f"python rascal_image_analyzer_headless.py")
        
        return filename
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        raise

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description="ğŸ¦ RASCALæ¥½å¤©ãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨æ›¸ãæ›ãˆç‰ˆï¼‰")
    parser.add_argument("--pages", type=int, default=10, help="å–å¾—ãƒšãƒ¼ã‚¸æ•° (æœ€å¤§10)")
    parser.add_argument("--with-images", action="store_true", help="ğŸ¨ ç”»åƒURLå–å¾—ï¼ˆRASCAL 3.0å¯¾å¿œï¼‰")
    args = parser.parse_args()
    
    print("ğŸ¦" + "="*50)
    print("ğŸ¦ RASCALæ¥½å¤©ã‚¹ãƒ¼ãƒ„ã‚±ãƒ¼ã‚¹ãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—ã‚·ã‚¹ãƒ†ãƒ ")
    if args.with_images:
        print("ğŸ¦ RASCAL 3.0 ç”»åƒåˆ†æå¯¾å¿œç‰ˆ")
    else:
        print("ğŸ¦ åŸºæœ¬ãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—ç‰ˆ")
    print("ğŸ¦" + "="*50)
    
    try:
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        df = fetch_ranking_data(args.pages, args.with_images)
        
        if len(df) == 0:
            print("âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return 1
        
        # çµ±è¨ˆè¡¨ç¤º
        display_statistics(df, args.with_images)
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        saved_file = save_data(df, args.with_images)
        
        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if args.with_images:
            print(f"\nğŸ¦ RASCAL 3.0 ç”»åƒåˆ†ææ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼")
            print(f"ğŸ¨ è‰²å½©åˆ†æã€ãƒ‡ã‚¶ã‚¤ãƒ³åˆ†é¡ã€é«˜ç´šæ„Ÿè©•ä¾¡ã€æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")
            print(f"ğŸ’ å¸‚å ´ã®çœŸå®Ÿã‚’ç”»åƒãƒ‡ãƒ¼ã‚¿ã§è§£æ˜ã—ã¾ã—ã‚‡ã†ï¼")
        
        print(f"\nğŸ¦ ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†ï¼RASCALåˆ†æã‚’ãŠæ¥½ã—ã¿ãã ã•ã„ï¼")
        return 0
        
    except Exception as e:
        print(f"\nâŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
