#!/usr/bin/env python3
"""
ğŸ¦ RASCAL TOP100 Analyzer - é«˜åº¦åˆ†ææ©Ÿèƒ½ç¶­æŒç‰ˆ
-----------------------------------
TOP100ã«çµã‚Šã¤ã¤é‡è¦ãªåˆ†ææ©Ÿèƒ½ã‚’ç¶­æŒ
- å®Ÿå£²ä¾¡æ ¼æ¨å®šåˆ†æ
- å•†å“åå“è³ªè©•ä¾¡
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¦åº¦åˆ†æ
- è©³ç´°ãªå¸‚å ´åˆ†æ
"""
import pandas as pd
import glob
import os
from datetime import datetime
import json
import re

class RASCALTOP100Analyzer:
    """TOP100å°‚ç”¨é«˜åº¦åˆ†æå™¨"""
    
    def __init__(self):
        self.data_dir = "data"
        self.changes_dir = os.path.join(self.data_dir, "changes")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¦åº¦è¨­å®š
        self.tier1_keywords = ["è»½é‡", "æ©Ÿå†…æŒã¡è¾¼ã¿", "ã‚­ãƒ£ãƒªãƒ¼ã‚±ãƒ¼ã‚¹"]
        self.tier2_keywords = ["TSAãƒ­ãƒƒã‚¯", "é™éŸ³", "USB"]
        self.tier3_keywords = ["ãƒ•ãƒ­ãƒ³ãƒˆã‚ªãƒ¼ãƒ—ãƒ³", "ã‚¹ãƒˆãƒƒãƒ‘ãƒ¼", "æ‹¡å¼µ"]
        
        os.makedirs(self.changes_dir, exist_ok=True)
    
    def find_latest_files(self):
        """æœ€æ–°2ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        csv_files = sorted(glob.glob(os.path.join(self.data_dir, "rank_base_*.csv")))
        if len(csv_files) < 2:
            raise FileNotFoundError("æ¯”è¼ƒç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³")
        return csv_files[-1], csv_files[-2]
    
    def load_data(self, filepath):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        return pd.read_csv(filepath)
    
    def analyze_price_structure(self, df):
        """ä¾¡æ ¼æ§‹é€ åˆ†æ"""
        price_stats = {
            "basic_stats": {
                "total_items": len(df),
                "avg_price": round(df['itemPrice'].mean(), 0),
                "median_price": round(df['itemPrice'].median(), 0),
                "min_price": int(df['itemPrice'].min()),
                "max_price": int(df['itemPrice'].max()),
                "price_std": round(df['itemPrice'].std(), 0)
            }
        }
        
        # ä¾¡æ ¼å¸¯åˆ†æ
        price_ranges = {
            "under_10k": len(df[df['itemPrice'] < 10000]),
            "10k_to_20k": len(df[(df['itemPrice'] >= 10000) & (df['itemPrice'] < 20000)]),
            "20k_to_50k": len(df[(df['itemPrice'] >= 20000) & (df['itemPrice'] < 50000)]),
            "over_50k": len(df[df['itemPrice'] >= 50000])
        }
        price_stats["price_ranges"] = price_ranges
        
        return price_stats
    
    def analyze_real_price_estimation(self, df):
        """å®Ÿå£²ä¾¡æ ¼æ¨å®šåˆ†æ"""
        if 'estimated_real_price' not in df.columns:
            return {"analysis": "å®Ÿå£²ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãªã—"}
        
        # ä¾¡æ ¼æ“ä½œæ¤œå‡º
        manipulated = df[df['itemPrice'] != df['estimated_real_price']]
        
        if len(manipulated) == 0:
            return {"price_manipulation_rate": 0, "examples": []}
        
        manipulation_rate = len(manipulated) / len(df) * 100
        avg_discount_amount = (df['itemPrice'] - df['estimated_real_price']).mean()
        avg_discount_rate = ((df['itemPrice'] - df['estimated_real_price']) / df['itemPrice'] * 100).mean()
        
        # æ“ä½œä¾‹ï¼ˆä¸Šä½5ä»¶ï¼‰
        examples = []
        for _, item in manipulated.head(5).iterrows():
            discount = item['itemPrice'] - item['estimated_real_price']
            discount_rate = (discount / item['itemPrice']) * 100
            examples.append({
                "rank": int(item['rank']),
                "display_price": int(item['itemPrice']),
                "estimated_real_price": int(item['estimated_real_price']),
                "discount_amount": int(discount),
                "discount_rate": round(discount_rate, 1)
            })
        
        return {
            "price_manipulation_rate": round(manipulation_rate, 1),
            "avg_discount_amount": round(avg_discount_amount, 0),
            "avg_discount_rate": round(avg_discount_rate, 1),
            "manipulated_count": len(manipulated),
            "examples": examples
        }
    
    def analyze_name_quality(self, df):
        """å•†å“åå“è³ªåˆ†æ"""
        if 'name_quality_score' not in df.columns:
            return {"analysis": "å“è³ªã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ãªã—"}
        
        quality_stats = {
            "avg_score": round(df['name_quality_score'].mean(), 1),
            "median_score": round(df['name_quality_score'].median(), 1),
            "min_score": int(df['name_quality_score'].min()),
            "max_score": int(df['name_quality_score'].max())
        }
        
        # å“è³ªåˆ†å¸ƒ
        quality_ranges = {
            "excellent_90_plus": len(df[df['name_quality_score'] >= 90]),
            "good_80_to_89": len(df[(df['name_quality_score'] >= 80) & (df['name_quality_score'] < 90)]),
            "average_70_to_79": len(df[(df['name_quality_score'] >= 70) & (df['name_quality_score'] < 80)]),
            "poor_under_70": len(df[df['name_quality_score'] < 70])
        }
        
        # æ–‡å­—æ•°åˆ†æ
        df['name_length'] = df['itemName'].str.len()
        length_stats = {
            "avg_length": round(df['name_length'].mean(), 1),
            "long_names_160_plus": len(df[df['name_length'] > 160]),
            "optimal_80_to_160": len(df[(df['name_length'] >= 80) & (df['name_length'] <= 160)]),
            "short_names_under_80": len(df[df['name_length'] < 80])
        }
        
        return {
            "quality_stats": quality_stats,
            "quality_distribution": quality_ranges,
            "length_analysis": length_stats
        }
    
    def analyze_keyword_importance(self, df):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¦åº¦åˆ†æ"""
        total_items = len(df)
        
        def count_keyword_occurrences(keywords, label):
            results = {}
            for keyword in keywords:
                count = df['itemName'].str.contains(keyword, case=False, na=False).sum()
                percentage = round((count / total_items) * 100, 1)
                results[keyword] = {"count": int(count), "percentage": percentage}
            return results
        
        keyword_analysis = {
            "tier1_high_importance": count_keyword_occurrences(self.tier1_keywords, "Tier1"),
            "tier2_medium_importance": count_keyword_occurrences(self.tier2_keywords, "Tier2"),
            "tier3_basic_importance": count_keyword_occurrences(self.tier3_keywords, "Tier3")
        }
        
        # æœ€ã‚‚é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é †ä½
        all_keywords = []
        for tier, keywords in keyword_analysis.items():
            for keyword, data in keywords.items():
                all_keywords.append((keyword, data["percentage"], tier))
        
        all_keywords.sort(key=lambda x: x[1], reverse=True)
        top_keywords = all_keywords[:10]
        
        return {
            "keyword_tiers": keyword_analysis,
            "top_keywords": [{"keyword": k[0], "percentage": k[1], "tier": k[2]} for k in top_keywords]
        }
    
    def analyze_feature_distribution(self, df):
        """æ©Ÿèƒ½åˆ†å¸ƒåˆ†æ"""
        feature_cols = [col for col in df.columns if col.startswith(('has_', 'is_', 'for_', 'appeal_'))]
        
        feature_analysis = {}
        total_items = len(df)
        
        for col in feature_cols:
            count = df[col].sum()
            percentage = round((count / total_items) * 100, 1)
            feature_name = col.replace('has_', '').replace('is_', '').replace('for_', '').replace('appeal_', '')
            feature_analysis[feature_name] = {
                "count": int(count),
                "percentage": percentage
            }
        
        # æ©Ÿèƒ½åˆ¥ã‚½ãƒ¼ãƒˆ
        sorted_features = sorted(feature_analysis.items(), key=lambda x: x[1]["percentage"], reverse=True)
        
        return {
            "all_features": feature_analysis,
            "top_features": dict(sorted_features[:15]),
            "feature_summary": {
                "total_features_tracked": len(feature_cols),
                "avg_feature_adoption": round(sum(f["percentage"] for f in feature_analysis.values()) / len(feature_analysis), 1)
            }
        }
    
    def analyze_market_changes(self, today_df, yesterday_df):
        """å¸‚å ´å¤‰åŒ–åˆ†æ"""
        yesterday_codes = set(yesterday_df['itemCode'])
        today_codes = set(today_df['itemCode'])
        
        # åŸºæœ¬å¤‰åŒ–çµ±è¨ˆ
        new_items = today_codes - yesterday_codes
        dropped_items = yesterday_codes - today_codes
        common_items = today_codes & yesterday_codes
        
        # ä¾¡æ ¼å¤‰å‹•åˆ†æï¼ˆç¶™ç¶šå•†å“ã®ã¿ï¼‰
        if len(common_items) > 0:
            common_df = pd.merge(
                today_df[['itemCode', 'itemPrice', 'rank']],
                yesterday_df[['itemCode', 'itemPrice']],
                on='itemCode',
                suffixes=('_today', '_yesterday')
            )
            
            price_changed = common_df[common_df['itemPrice_today'] != common_df['itemPrice_yesterday']]
            
            price_change_analysis = {
                "items_with_price_change": len(price_changed),
                "price_change_rate": round(len(price_changed) / len(common_df) * 100, 1) if len(common_df) > 0 else 0
            }
        else:
            price_change_analysis = {"items_with_price_change": 0, "price_change_rate": 0}
        
        return {
            "market_flow": {
                "new_entries": len(new_items),
                "dropped_items": len(dropped_items),
                "continuing_items": len(common_items),
                "turnover_rate": round((len(new_items) + len(dropped_items)) / len(yesterday_df) * 100, 1)
            },
            "price_dynamics": price_change_analysis
        }
    
    def display_top10_products_full(self, df):
        """TOP10å•†å“åã‚’å®Œå…¨ç‰ˆã§è¡¨ç¤º"""
        print(f"\nğŸ† TOP10å•†å“ä¸€è¦§ï¼ˆå®Œå…¨ç‰ˆï¼‰")
        print("="*100)
        
        top10 = df.head(10)
        for _, item in top10.iterrows():
            rank = item['rank']
            name = item['itemName']
            price = item['itemPrice']
            review_avg = item['reviewAverage']
            review_count = item['reviewCount']
            
            print(f"{rank:2d}ä½: {name}")
            print(f"      Â¥{price:,} | â­{review_avg:.1f} ({review_count:,}ä»¶)")
            print("")
        
        # å•†å“åçµ±è¨ˆã‚‚è¿½åŠ 
        name_lengths = [len(item['itemName']) for _, item in top10.iterrows()]
        avg_length = sum(name_lengths) / len(name_lengths)
        
        print(f"ğŸ“ TOP10å•†å“åçµ±è¨ˆ:")
        print(f"  å¹³å‡æ–‡å­—æ•°: {avg_length:.1f}æ–‡å­—")
        print(f"  æœ€é•·: {max(name_lengths)}æ–‡å­—")
        print(f"  æœ€çŸ­: {min(name_lengths)}æ–‡å­—")
    
    def generate_comprehensive_analysis(self):
        """åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ"""
        print("ğŸ¦ RASCAL TOP100 åŒ…æ‹¬åˆ†æé–‹å§‹...")
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
            today_file, yesterday_file = self.find_latest_files()
            today_df = self.load_data(today_file)
            yesterday_df = self.load_data(yesterday_file)
            
            print(f"ğŸ“Š åˆ†æå¯¾è±¡: TOP{len(today_df)}å•†å“")
            
            # å„ç¨®åˆ†æå®Ÿè¡Œ
            print("ğŸ’° ä¾¡æ ¼æ§‹é€ åˆ†æä¸­...")
            price_analysis = self.analyze_price_structure(today_df)
            
            print("ğŸ” å®Ÿå£²ä¾¡æ ¼æ¨å®šåˆ†æä¸­...")
            real_price_analysis = self.analyze_real_price_estimation(today_df)
            
            print("ğŸ“ å•†å“åå“è³ªåˆ†æä¸­...")
            name_quality_analysis = self.analyze_name_quality(today_df)
            
            print("ğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¦åº¦åˆ†æä¸­...")
            keyword_analysis = self.analyze_keyword_importance(today_df)
            
            print("âš™ï¸ æ©Ÿèƒ½åˆ†å¸ƒåˆ†æä¸­...")
            feature_analysis = self.analyze_feature_distribution(today_df)
            
            print("ğŸ“ˆ å¸‚å ´å¤‰åŒ–åˆ†æä¸­...")
            market_change_analysis = self.analyze_market_changes(today_df, yesterday_df)
            
            # çµæœçµ±åˆ
            comprehensive_result = {
                "analysis_timestamp": datetime.now().isoformat(),
                "files_analyzed": {
                    "current": os.path.basename(today_file),
                    "previous": os.path.basename(yesterday_file)
                },
                "price_analysis": price_analysis,
                "real_price_estimation": real_price_analysis,
                "name_quality_analysis": name_quality_analysis,
                "keyword_importance": keyword_analysis,
                "feature_distribution": feature_analysis,
                "market_changes": market_change_analysis,
                "analysis_summary": {
                    "total_items_analyzed": len(today_df),
                    "analysis_scope": "TOP100",
                    "key_features_tracked": 30,
                    "advanced_analytics_enabled": True
                }
            }
            
            # çµæœä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M")
            output_file = os.path.join(self.changes_dir, f"top100_analysis_{timestamp}.json")
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(comprehensive_result, f, ensure_ascii=False, indent=2)
            
            # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
            self.print_analysis_report(comprehensive_result)
            
            print(f"\nâœ… åŒ…æ‹¬åˆ†æå®Œäº†: {output_file}")
            return output_file
            
        except Exception as e:
            print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def print_analysis_report(self, result):
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        print("\n" + "="*50)
        print("ğŸ¦ RASCAL TOP100 åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*50)
        
        # ä¾¡æ ¼åˆ†æ
        price = result["price_analysis"]["basic_stats"]
        print(f"ğŸ’° ä¾¡æ ¼åˆ†æ")
        print(f"  å¹³å‡ä¾¡æ ¼: Â¥{price['avg_price']:,.0f}")
        print(f"  ä¸­å¤®å€¤: Â¥{price['median_price']:,.0f}")
        print(f"  ä¾¡æ ¼ç¯„å›²: Â¥{price['min_price']:,} - Â¥{price['max_price']:,}")
        
        # å®Ÿå£²ä¾¡æ ¼æ¨å®š
        real_price = result["real_price_estimation"]
        if "price_manipulation_rate" in real_price:
            print(f"\nğŸ” å®Ÿå£²ä¾¡æ ¼æ¨å®š")
            print(f"  ä¾¡æ ¼æ“ä½œæ¤œå‡ºç‡: {real_price['price_manipulation_rate']}%")
            print(f"  å¹³å‡å‰²å¼•é¡: Â¥{real_price['avg_discount_amount']:,.0f}")
            print(f"  å¹³å‡å‰²å¼•ç‡: {real_price['avg_discount_rate']}%")
        
        # å•†å“åå“è³ª
        quality = result["name_quality_analysis"]
        if "quality_stats" in quality:
            stats = quality["quality_stats"]
            print(f"\nğŸ“ å•†å“åå“è³ª")
            print(f"  å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {stats['avg_score']}ç‚¹")
            print(f"  é«˜å“è³ªå•†å“(90ç‚¹ä»¥ä¸Š): {quality['quality_distribution']['excellent_90_plus']}ä»¶")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¦åº¦
        keywords = result["keyword_importance"]["top_keywords"]
        print(f"\nğŸ·ï¸ é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ TOP5")
        for i, kw in enumerate(keywords[:5], 1):
            print(f"  {i}. {kw['keyword']}: {kw['percentage']}% ({kw['tier']})")
        
        # æ©Ÿèƒ½åˆ†å¸ƒ
        features = result["feature_distribution"]["top_features"]
        print(f"\nâš™ï¸ ä¸»è¦æ©Ÿèƒ½ TOP5")
        for i, (feature, data) in enumerate(list(features.items())[:5], 1):
            print(f"  {i}. {feature}: {data['count']}ä»¶ ({data['percentage']}%)")
        
        # å¸‚å ´å¤‰åŒ–
        changes = result["market_changes"]["market_flow"]
        print(f"\nğŸ“ˆ å¸‚å ´å¤‰åŒ–")
        print(f"  æ–°è¦å‚å…¥: {changes['new_entries']}ä»¶")
        print(f"  ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¢ã‚¦ãƒˆ: {changes['dropped_items']}ä»¶")
        print(f"  ç¶™ç¶šå•†å“: {changes['continuing_items']}ä»¶")
        print(f"  å¸‚å ´æµå‹•æ€§: {changes['turnover_rate']}%")

if __name__ == "__main__":
    analyzer = RASCALTOP100Analyzer()
    analyzer.generate_comprehensive_analysis()
